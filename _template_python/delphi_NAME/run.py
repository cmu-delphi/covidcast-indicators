# -*- coding: utf-8 -*-
"""Functions to call when running the function.

This module should contain a function called `run_module`, that is executed
when the module is run with `python -m MODULE_NAME`.  `run_module`'s lone argument should be a
nested dictionary of parameters loaded from the params.json file.  We expect the params to have
the following structure:
    - "common":
        - "export_dir": str, directory to which the output files are saved
        - "log_filename": (optional) str, path to log file
    - "indicator": (optional)
        - "wip_signal": (optional) Any[str, bool], list of signals that are works in progress, or
            True if all signals in the registry are works in progress, or False if only
            unpublished signals are.  See `delphi_utils.add_prefix()`
        - Any other indicator-specific settings
    - "patch": Only used for patching data, remove if not patching.
               Check patch.py and README for more details on how to run patches.
        - "start_date": str, YYYY-MM-DD format, first issue date
        - "end_date": str, YYYY-MM-DD format, last issue date
        - "patch_dir": str, directory to write all issues output


Other indicator-specific params can be useful for doing limited runs of the
indicator, for example for manual or unit testing, or local runs
(e.g. limiting the number of days of data to produce). Such params might
include:
    - "input_dir": str, path to aggregated doctor-visits data.
    - "drop_date": str, YYYY-MM-DD format, date data is dropped. If set to
       empty string, current day minus 40 hours is used.
    - "n_backfill_days": int, number of past days to generate estimates for.
    - "n_waiting_days": int, number of most recent days to skip estimates for.
    - "weekday": list of bool, which weekday adjustments to perform. For each value in the
        list, signals will be generated with weekday adjustments (True) or without
        adjustments (False)
    - "se": bool, whether to write out standard errors
    - "obfuscated_prefix": str, prefix for signal name if write_se is True.
    - "parallel": bool, whether to run in parallel.
    - "export_start_date": str, YYYY-MM-DD format, date from which to export data
    - "num_export_days": int, number of days before end date (today) to export
    - "path_to_bigquery_credentials": str, path to BigQuery API key and service account
        JSON file
    - Other credentials (as secrets)
    - "test_file" (optional): str, name of file from which to read test data
    - "socrata_token": str, authentication for upstream data pull
"""

import time
from datetime import timedelta, datetime
from itertools import product

import pandas as pd
from delphi_utils import get_structured_logger
from delphi_utils.export import create_export_csv
from delphi_utils.geomap import GeoMapper

from .constants import GEOS, SIGNALS, SMOOTHERS, CSV_COLS
from .pull import fetch_data
from .utils import summary_log, add_needed_columns


def run_module(params, logger=None):
    """
    Run the indicator

    Arguments
    --------
    params:  Dict[str, Any]
        Nested dictionary of parameters containing indicator configuration. See top
        of this file for details.
    logger:
        Optional logger object to use instead of that autogenerated in `run_module`.
    """
    start_time = time.time()
    issue_date = params.get("patch", {}).get("current_issue", None)

    export_dir = params["common"]["export_dir"]
    # If authentication required, include as a secret in params.json
    api_key = params["indicator"]["api_key"]

    logger = get_structured_logger(
        __name__, filename=params["common"].get("log_filename"),
        log_exceptions=params["common"].get("log_exceptions", True))

    run_stats = []

    # Build the base version of the signal at the most detailed geo level
    # possible (usually zip or county).
    base_geo = "zip"
    all_data = fetch_data(api_key)

    mapper = GeoMapper()
    # Compute variations. Add num/prop variations if needed
    for signal, smoother_name, geo in product(SIGNALS, SMOOTHERS, GEOS):
        logger.info("Generating signal and exporting to CSV",
            geo_res = geo,
            metric=signal,
            smoother = smoother_name,
        )

        df = all_data.copy()
        df["val"] = df[signal]

        # Sometimes a source will report at several different geo levels
        # (e.g. county and nation), but not all geo levels that we want. In
        # that case, separate out processing by geo for those explicitly
        # provided and use geomapper-based aggregation on everything else.
        # Example:
        #   if geo == "nation":
        #       df = df[df["geography"] == "United States"]
        #       df["geo_id"] = "us"

        # Aggregate using sum from base_geo to new geo. Weighted sum also available.
        df = mapper.replace_geocode(all_data, base_geo, geo, new_col="geo_id")

        # Recompute sample_size, se here if not NA

        smoother = SMOOTHERS_MAP[smoother_name]
        signal_name = signal + smoother[0] # Add "num"/"prop" to name if used
        df["val"] = df[["geo_id", "val"]].groupby("geo_id")["val"].transform(
            smoother[1].smooth
        )

        start_date = min(df.timestamp)
        if smoother_name == "smoothed":
            # Don't export first n-1 days for smoothed signals since they'll
            # be NaN/missing. Usually n = 7, since our standard smoother is a
            # rolling 7-day average.
            start_date += timedelta(6)

        # Add se, sample_size, and NaN codes
        missing_cols = set(CSV_COLS) - set(df.columns)
        df = add_needed_columns(df, col_names=list(missing_cols))
        df = df[CSV_COLS + ["timestamp"]]

        if len(df) == 0:
            continue
        exported_csv_dates = create_export_csv(
            df,
            export_dir,
            geo,
            signal_name,
            start_date=start_date)

        if len(exported_csv_dates) > 0:
            run_stats.append((max(exported_csv_dates), len(exported_csv_dates)))

    # Log this indicator run
    summary_log(start_time, run_stats, logger)
